{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CMO808/DCGAN-TF2.0/blob/master/Untitled2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from function import conv2d, deconv2d, lrelu, fc, embedding_lookup\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "def Generator(images, En, De, embeddings, embedding_ids, GPU=False, encode_layers=False):\n",
        "    encoded_source, encode_layers = En(images)\n",
        "    local_embeddings = embedding_lookup(embeddings, embedding_ids, GPU=GPU)\n",
        "    if GPU:\n",
        "        encoded_source = encoded_source.cuda()\n",
        "        local_embeddings = local_embeddings.cuda()\n",
        "    embedded = torch.cat((encoded_source, local_embeddings), 1)\n",
        "    fake_target = De(embedded, encode_layers)\n",
        "    if encode_layers:\n",
        "        return fake_target, encoded_source, encode_layers\n",
        "    else:\n",
        "        return fake_target, encoded_source\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, img_dim=1, conv_dim=64):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.conv1 = conv2d(img_dim, conv_dim, k_size=5, stride=2, pad=2, dilation=2, lrelu=False, bn=False)\n",
        "        self.conv2 = conv2d(conv_dim, conv_dim*2, k_size=5, stride=2, pad=2, dilation=2)\n",
        "        self.conv3 = conv2d(conv_dim*2, conv_dim*4, k_size=4, stride=2, pad=1, dilation=1)\n",
        "        self.conv4 = conv2d(conv_dim*4, conv_dim*8)\n",
        "        self.conv5 = conv2d(conv_dim*8, conv_dim*8)\n",
        "        self.conv6 = conv2d(conv_dim*8, conv_dim*8)\n",
        "        self.conv7 = conv2d(conv_dim*8, conv_dim*8)\n",
        "        self.conv8 = conv2d(conv_dim*8, conv_dim*8)\n",
        "    \n",
        "    def forward(self, images):\n",
        "        encode_layers = dict()\n",
        "        \n",
        "        e1 = self.conv1(images)\n",
        "        encode_layers['e1'] = e1\n",
        "        e2 = self.conv2(e1)\n",
        "        encode_layers['e2'] = e2\n",
        "        e3 = self.conv3(e2)\n",
        "        encode_layers['e3'] = e3\n",
        "        e4 = self.conv4(e3)\n",
        "        encode_layers['e4'] = e4\n",
        "        e5 = self.conv5(e4)\n",
        "        encode_layers['e5'] = e5\n",
        "        e6 = self.conv6(e5)\n",
        "        encode_layers['e6'] = e6\n",
        "        e7 = self.conv7(e6)\n",
        "        encode_layers['e7'] = e7\n",
        "        encoded_source = self.conv8(e7)\n",
        "        encode_layers['e8'] = encoded_source\n",
        "        \n",
        "        return encoded_source, encode_layers\n",
        "    \n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, img_dim=1, embedded_dim=640, conv_dim=64):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.deconv1 = deconv2d(embedded_dim, conv_dim*8, dropout=True)\n",
        "        self.deconv2 = deconv2d(conv_dim*16, conv_dim*8, dropout=True, k_size=4)\n",
        "        self.deconv3 = deconv2d(conv_dim*16, conv_dim*8, k_size=5, dilation=2, dropout=True)\n",
        "        self.deconv4 = deconv2d(conv_dim*16, conv_dim*8, k_size=4, dilation=2, stride=2)\n",
        "        self.deconv5 = deconv2d(conv_dim*16, conv_dim*4, k_size=4, dilation=2, stride=2)\n",
        "        self.deconv6 = deconv2d(conv_dim*8, conv_dim*2, k_size=4, dilation=2, stride=2)\n",
        "        self.deconv7 = deconv2d(conv_dim*4, conv_dim*1, k_size=4, dilation=2, stride=2)\n",
        "        self.deconv8 = deconv2d(conv_dim*2, img_dim, k_size=4, dilation=2, stride=2, bn=False)\n",
        "    \n",
        "    \n",
        "    def forward(self, embedded, encode_layers):\n",
        "        \n",
        "        d1 = self.deconv1(embedded)\n",
        "        d1 = torch.cat((d1, encode_layers['e7']), dim=1)\n",
        "        d2 = self.deconv2(d1)\n",
        "        d2 = torch.cat((d2, encode_layers['e6']), dim=1)\n",
        "        d3 = self.deconv3(d2)\n",
        "        d3 = torch.cat((d3, encode_layers['e5']), dim=1)\n",
        "        d4 = self.deconv4(d3)\n",
        "        d4 = torch.cat((d4, encode_layers['e4']), dim=1)\n",
        "        d5 = self.deconv5(d4)\n",
        "        d5 = torch.cat((d5, encode_layers['e3']), dim=1)\n",
        "        d6 = self.deconv6(d5)\n",
        "        d6 = torch.cat((d6, encode_layers['e2']), dim=1)\n",
        "        d7 = self.deconv7(d6)\n",
        "        d7 = torch.cat((d7, encode_layers['e1']), dim=1)\n",
        "        d8 = self.deconv8(d7)        \n",
        "        fake_target = torch.tanh(d8)\n",
        "        \n",
        "        return fake_target\n",
        "    \n",
        "    \n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, category_num, img_dim=2, disc_dim=64):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.conv1 = conv2d(img_dim, disc_dim, bn=False)\n",
        "        self.conv2 = conv2d(disc_dim, disc_dim*2)\n",
        "        self.conv3 = conv2d(disc_dim*2, disc_dim*4)\n",
        "        self.conv4 = conv2d(disc_dim*4, disc_dim*8)\n",
        "        self.fc1 = fc(disc_dim*8*8*8, 1)\n",
        "        self.fc2 = fc(disc_dim*8*8*8, category_num)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        batch_size = images.shape[0]\n",
        "        h1 = self.conv1(images)\n",
        "        h2 = self.conv2(h1)\n",
        "        h3 = self.conv3(h2)\n",
        "        h4 = self.conv4(h3)\n",
        "        \n",
        "        tf_loss_logit = self.fc1(h4.reshape(batch_size, -1))\n",
        "        tf_loss = torch.sigmoid(tf_loss_logit)\n",
        "        cat_loss = self.fc2(h4.reshape(batch_size, -1))\n",
        "        \n",
        "        return tf_loss, tf_loss_logit, cat_loss\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "TWTHgJcgPTdh",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "89773528-fae5-41c8-c667-312ba888b6b2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-efa33c555988>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfunction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeconv2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlrelu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_lookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'function'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, time, datetime\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "from common.dataset import TrainDataProvider\n",
        "from common.function import init_embedding\n",
        "from common.models import Encoder, Decoder, Discriminator, Generator\n",
        "from common.utils import denorm_image, centering_image\n",
        "\n",
        "\n",
        "class Trainer:\n",
        "    \n",
        "    def __init__(self, GPU, data_dir, fixed_dir, fonts_num, batch_size, img_size):\n",
        "        self.GPU = GPU\n",
        "        self.data_dir = data_dir\n",
        "        self.fixed_dir = fixed_dir\n",
        "        self.fonts_num = fonts_num\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        \n",
        "        self.embeddings = torch.load(os.path.join(fixed_dir, 'EMBEDDINGS.pkl'))\n",
        "        self.embedding_num = self.embeddings.shape[0]\n",
        "        self.embedding_dim = self.embeddings.shape[3]\n",
        "        \n",
        "        self.fixed_source = torch.load(os.path.join(fixed_dir, 'fixed_source.pkl'))\n",
        "        self.fixed_target = torch.load(os.path.join(fixed_dir, 'fixed_target.pkl'))\n",
        "        self.fixed_label = torch.load(os.path.join(fixed_dir, 'fixed_label.pkl'))\n",
        "        \n",
        "        self.data_provider = TrainDataProvider(self.data_dir)\n",
        "        self.total_batches = self.data_provider.compute_total_batch_num(self.batch_size)\n",
        "        print(\"total batches:\", self.total_batches)\n",
        "\n",
        "\n",
        "    def train(self, max_epoch, schedule, save_path, to_model_path, lr=0.001, \\\n",
        "              log_step=100, sample_step=350, fine_tune=False, flip_labels=False, \\\n",
        "              restore=None, from_model_path=False, with_charid=False, \\\n",
        "              freeze_encoder=False, save_nrow=8, model_save_step=None, resize_fix=90):\n",
        "\n",
        "        # Fine Tuning coefficient\n",
        "        if not fine_tune:\n",
        "            L1_penalty, Lconst_penalty = 100, 15\n",
        "        else:\n",
        "            L1_penalty, Lconst_penalty = 500, 1000\n",
        "\n",
        "        # Get Models\n",
        "        En = Encoder()\n",
        "        De = Decoder()\n",
        "        D = Discriminator(category_num=self.fonts_num)\n",
        "        if self.GPU:\n",
        "            En.cuda()\n",
        "            De.cuda()\n",
        "            D.cuda()\n",
        "\n",
        "        # Use pre-trained Model\n",
        "        # restore에 [encoder_path, decoder_path, discriminator_path] 형태로 인자 넣기\n",
        "        if restore:\n",
        "            encoder_path, decoder_path, discriminator_path = restore\n",
        "            prev_epoch = int(encoder_path.split('-')[0])\n",
        "            En.load_state_dict(torch.load(os.path.join(from_model_path, encoder_path)))\n",
        "            De.load_state_dict(torch.load(os.path.join(from_model_path, decoder_path)))\n",
        "            D.load_state_dict(torch.load(os.path.join(from_model_path, discriminator_path)))\n",
        "            print(\"%d epoch trained model has restored\" % prev_epoch)\n",
        "        else:\n",
        "            prev_epoch = 0\n",
        "            print(\"New model training start\")\n",
        "\n",
        "\n",
        "        # L1 loss, binary real/fake loss, category loss, constant loss\n",
        "        if self.GPU:\n",
        "            l1_criterion = nn.L1Loss(size_average=True).cuda()\n",
        "            bce_criterion = nn.BCEWithLogitsLoss(size_average=True).cuda()\n",
        "            mse_criterion = nn.MSELoss(size_average=True).cuda()\n",
        "        else:\n",
        "            l1_criterion = nn.L1Loss(size_average=True)\n",
        "            bce_criterion = nn.BCEWithLogitsLoss(size_average=True)\n",
        "            mse_criterion = nn.MSELoss(size_average=True)\n",
        "\n",
        "\n",
        "        # optimizer\n",
        "        if freeze_encoder:\n",
        "            G_parameters = list(De.parameters())\n",
        "        else:\n",
        "            G_parameters = list(En.parameters()) + list(De.parameters())\n",
        "        g_optimizer = torch.optim.Adam(G_parameters, betas=(0.5, 0.999))\n",
        "        d_optimizer = torch.optim.Adam(D.parameters(), betas=(0.5, 0.999))\n",
        "\n",
        "        # losses lists\n",
        "        l1_losses, const_losses, category_losses, d_losses, g_losses = list(), list(), list(), list(), list()\n",
        "\n",
        "        # training\n",
        "        count = 0\n",
        "        for epoch in range(max_epoch):\n",
        "            if (epoch + 1) % schedule == 0:\n",
        "                updated_lr = max(lr/2, 0.0002)\n",
        "                for param_group in d_optimizer.param_groups:\n",
        "                    param_group['lr'] = updated_lr\n",
        "                for param_group in g_optimizer.param_groups:\n",
        "                    param_group['lr'] = updated_lr\n",
        "                if lr !=  updated_lr:\n",
        "                    print(\"decay learning rate from %.5f to %.5f\" % (lr, updated_lr))\n",
        "                lr = updated_lr\n",
        "\n",
        "            train_batch_iter = self.data_provider.get_train_iter(self.batch_size, \\\n",
        "                                                            with_charid=with_charid)   \n",
        "            for i, batch in enumerate(train_batch_iter):\n",
        "                if with_charid:\n",
        "                    font_ids, char_ids, batch_images = batch\n",
        "                else:\n",
        "                    font_ids, batch_images = batch\n",
        "                embedding_ids = font_ids\n",
        "                if self.GPU:\n",
        "                    batch_images = batch_images.cuda()\n",
        "                if flip_labels:\n",
        "                    np.random.shuffle(embedding_ids)\n",
        "\n",
        "                # target / source images\n",
        "                real_target = batch_images[:, 0, :, :]\n",
        "                real_target = real_target.view([self.batch_size, 1, self.img_size, self.img_size])\n",
        "                real_source = batch_images[:, 1, :, :]\n",
        "                real_source = real_source.view([self.batch_size, 1, self.img_size, self.img_size])\n",
        "                \n",
        "                # centering\n",
        "                for idx, (image_S, image_T) in enumerate(zip(real_source, real_target)):\n",
        "                    image_S = image_S.cpu().detach().numpy().reshape(self.img_size, self.img_size)\n",
        "                    image_S = centering_image(image_S, resize_fix=90)\n",
        "                    real_source[idx] = torch.tensor(image_S).view([1, self.img_size, self.img_size])\n",
        "                    image_T = image_T.cpu().detach().numpy().reshape(self.img_size, self.img_size)\n",
        "                    image_T = centering_image(image_T, resize_fix=resize_fix)\n",
        "                    real_target[idx] = torch.tensor(image_T).view([1, self.img_size, self.img_size])\n",
        "\n",
        "                # generate fake image form source image\n",
        "                fake_target, encoded_source, _ = Generator(real_source, En, De, \\\n",
        "                                                           self.embeddings, embedding_ids, \\\n",
        "                                                           GPU=self.GPU, encode_layers=True)\n",
        "\n",
        "                real_TS = torch.cat([real_source, real_target], dim=1)\n",
        "                fake_TS = torch.cat([real_source, fake_target], dim=1)\n",
        "\n",
        "                # Scoring with Discriminator\n",
        "                real_score, real_score_logit, real_cat_logit = D(real_TS)\n",
        "                fake_score, fake_score_logit, fake_cat_logit = D(fake_TS)\n",
        "\n",
        "                # Get encoded fake image to calculate constant loss\n",
        "                encoded_fake = En(fake_target)[0]\n",
        "                const_loss = Lconst_penalty * mse_criterion(encoded_source, encoded_fake)\n",
        "\n",
        "                # category loss\n",
        "                real_category = torch.from_numpy(np.eye(self.fonts_num)[embedding_ids]).float()\n",
        "                if self.GPU:\n",
        "                    real_category = real_category.cuda()\n",
        "                real_category_loss = bce_criterion(real_cat_logit, real_category)\n",
        "                fake_category_loss = bce_criterion(fake_cat_logit, real_category)\n",
        "                category_loss = 0.5 * (real_category_loss + fake_category_loss)\n",
        "\n",
        "                # labels\n",
        "                if self.GPU:\n",
        "                    one_labels = torch.ones([self.batch_size, 1]).cuda()\n",
        "                    zero_labels = torch.zeros([self.batch_size, 1]).cuda()\n",
        "                else:\n",
        "                    one_labels = torch.ones([self.batch_size, 1])\n",
        "                    zero_labels = torch.zeros([self.batch_size, 1])\n",
        "\n",
        "                # binary loss - T/F\n",
        "                real_binary_loss = bce_criterion(real_score_logit, one_labels)\n",
        "                fake_binary_loss = bce_criterion(fake_score_logit, zero_labels)\n",
        "                binary_loss = real_binary_loss + fake_binary_loss\n",
        "\n",
        "                # L1 loss between real and fake images\n",
        "                l1_loss = L1_penalty * l1_criterion(real_target, fake_target)\n",
        "\n",
        "                # cheat loss for generator to fool discriminator\n",
        "                cheat_loss = bce_criterion(fake_score_logit, one_labels)\n",
        "\n",
        "                # g_loss, d_loss\n",
        "                g_loss = cheat_loss + l1_loss + fake_category_loss + const_loss\n",
        "                d_loss = binary_loss + category_loss\n",
        "\n",
        "                # train Discriminator\n",
        "                D.zero_grad()\n",
        "                d_loss.backward(retain_graph=True)\n",
        "                d_optimizer.step()\n",
        "\n",
        "                # train Generator\n",
        "                En.zero_grad()\n",
        "                De.zero_grad()\n",
        "                g_loss.backward(retain_graph=True)\n",
        "                g_optimizer.step()            \n",
        "\n",
        "                # loss data\n",
        "                l1_losses.append(int(l1_loss.data))\n",
        "                const_losses.append(int(const_loss.data))\n",
        "                category_losses.append(int(category_loss.data))\n",
        "                d_losses.append(int(d_loss.data))\n",
        "                g_losses.append(int(g_loss.data))\n",
        "\n",
        "                # logging\n",
        "                if (i+1) % log_step == 0:\n",
        "                    time_ = time.time()\n",
        "                    time_stamp = datetime.datetime.fromtimestamp(time_).strftime('%H:%M:%S')\n",
        "                    log_format = 'Epoch [%d/%d], step [%d/%d], l1_loss: %.4f, d_loss: %.4f, g_loss: %.4f' % \\\n",
        "                                 (int(prev_epoch)+epoch+1, int(prev_epoch)+max_epoch, \\\n",
        "                                  i+1, self.total_batches, l1_loss.item(), d_loss.item(), g_loss.item())\n",
        "                    print(time_stamp, log_format)\n",
        "\n",
        "                # save image\n",
        "                if (i+1) % sample_step == 0:\n",
        "                    fixed_fake_images = Generator(self.fixed_source, En, De, \\\n",
        "                                                  self.embeddings, self.fixed_label, GPU=self.GPU)[0]\n",
        "                    save_image(denorm_image(fixed_fake_images.data), \\\n",
        "                               os.path.join(save_path, 'fake_samples-%d-%d.png' % \\\n",
        "                                            (int(prev_epoch)+epoch+1, i+1)), \\\n",
        "                               nrow=save_nrow, pad_value=255)\n",
        "\n",
        "            if not model_save_step:\n",
        "                model_save_step = 5\n",
        "            if (epoch+1) % model_save_step == 0:\n",
        "                now = datetime.datetime.now()\n",
        "                now_date = now.strftime(\"%m%d\")\n",
        "                now_time = now.strftime('%H:%M')\n",
        "                torch.save(En.state_dict(), os.path.join(to_model_path, \\\n",
        "                                                         '%d-%s-%s-Encoder.pkl' % \\\n",
        "                                                         (int(prev_epoch)+epoch+1, \\\n",
        "                                                          now_date, now_time)))\n",
        "                torch.save(De.state_dict(), os.path.join(to_model_path, \\\n",
        "                                                         '%d-%s-%s-Decoder.pkl' % \\\n",
        "                                                         (int(prev_epoch)+epoch+1, \\\n",
        "                                                          now_date, now_time)))\n",
        "                torch.save(D.state_dict(), os.path.join(to_model_path, \\\n",
        "                                                        '%d-%s-%s-Discriminator.pkl' % \\\n",
        "                                                        (int(prev_epoch)+epoch+1, \\\n",
        "                                                         now_date, now_time)))\n",
        "\n",
        "        # save model\n",
        "        total_epoch = int(prev_epoch) + int(max_epoch)\n",
        "        end = datetime.datetime.now()\n",
        "        end_date = end.strftime(\"%m%d\")\n",
        "        end_time = end.strftime('%H:%M')\n",
        "        torch.save(En.state_dict(), os.path.join(to_model_path, \\\n",
        "                                                 '%d-%s-%s-Encoder.pkl' % \\\n",
        "                                                 (total_epoch, end_date, end_time)))\n",
        "        torch.save(De.state_dict(), os.path.join(to_model_path, \\\n",
        "                                                 '%d-%s-%s-Decoder.pkl' % \\\n",
        "                                                 (total_epoch, end_date, end_time)))\n",
        "        torch.save(D.state_dict(), os.path.join(to_model_path, \\\n",
        "                                                '%d-%s-%s-Discriminator.pkl' % \\\n",
        "                                                (total_epoch, end_date, end_time)))\n",
        "        losses = [l1_losses, const_losses, category_losses, d_losses, g_losses]\n",
        "        torch.save(losses, os.path.join(to_model_path, '%d-losses.pkl' % total_epoch))\n",
        "\n",
        "        return l1_losses, const_losses, category_losses, d_losses, g_losses\n",
        "\n",
        "\n",
        "def interpolation(data_provider, grids, fixed_char_ids, interpolated_font_ids, embeddings, \\\n",
        "                  En, De, batch_size, img_size=128, save_nrow=6, save_path=False, GPU=True):\n",
        "    \n",
        "    train_batch_iter = data_provider.get_train_iter(batch_size, with_charid=True)\n",
        "    \n",
        "    for grid_idx, grid in enumerate(grids):\n",
        "        train_batch_iter = data_provider.get_train_iter(batch_size, with_charid=True)\n",
        "        grid_results = {from_to: {charid: None for charid in fixed_char_ids} \\\n",
        "                        for from_to in interpolated_font_ids}\n",
        "\n",
        "        for i, batch in enumerate(train_batch_iter):\n",
        "            font_ids_from, char_ids, batch_images = batch\n",
        "            font_filter = [i[0] for i in interpolated_font_ids]\n",
        "            font_filter_plus = font_filter + [font_filter[0]]\n",
        "            font_ids_to = [font_filter_plus[font_filter.index(i)+1] for i in font_ids_from]\n",
        "            batch_images = batch_images.cuda()\n",
        "\n",
        "            real_sources = batch_images[:, 1, :, :].view([batch_size, 1, img_size, img_size])\n",
        "            real_targets = batch_images[:, 0, :, :].view([batch_size, 1, img_size, img_size])\n",
        "\n",
        "            for idx, (image_S, image_T) in enumerate(zip(real_sources, real_targets)):\n",
        "                image_S = image_S.cpu().detach().numpy().reshape(img_size, img_size)\n",
        "                image_S = centering_image(image_S, resize_fix=100)\n",
        "                real_sources[idx] = torch.tensor(image_S).view([1, img_size, img_size])\n",
        "                image_T = image_T.cpu().detach().numpy().reshape(img_size, img_size)\n",
        "                image_T = centering_image(image_T, resize_fix=100)\n",
        "                real_targets[idx] = torch.tensor(image_T).view([1, img_size, img_size])\n",
        "                \n",
        "            encoded_source, encode_layers = En(real_sources)\n",
        "\n",
        "            interpolated_embeddings = []\n",
        "            embedding_dim = embeddings.shape[3]\n",
        "            for from_, to_ in zip(font_ids_from, font_ids_to):\n",
        "                interpolated_embeddings.append((embeddings[from_] * (1 - grid) + \\\n",
        "                                                embeddings[to_] * grid).cpu().numpy())\n",
        "            interpolated_embeddings = torch.tensor(interpolated_embeddings).cuda()\n",
        "            interpolated_embeddings = interpolated_embeddings.reshape(batch_size, embedding_dim, 1, 1)\n",
        "\n",
        "            # generate fake image with embedded source\n",
        "            interpolated_embedded = torch.cat((encoded_source, interpolated_embeddings), 1)\n",
        "            fake_targets = De(interpolated_embedded, encode_layers)\n",
        "\n",
        "            # [(0)real_S, (1)real_T, (2)fake_T]\n",
        "            for fontid, charid, real_S, real_T, fake_T in zip(font_ids_from, char_ids, \\\n",
        "                                                              real_sources, real_targets, \\\n",
        "                                                              fake_targets):\n",
        "                font_from = fontid\n",
        "                font_to = font_filter_plus[font_filter.index(fontid)+1]\n",
        "                from_to = (font_from, font_to)\n",
        "                grid_results[from_to][charid] = [real_S, real_T, fake_T]\n",
        "\n",
        "        if save_path:\n",
        "            for from_to in grid_results.keys():\n",
        "                image = [grid_results[from_to][charid][2].cpu().detach().numpy() for \\\n",
        "                         charid in fixed_char_ids]\n",
        "                image = torch.tensor(np.array(image))\n",
        "\n",
        "                # path\n",
        "                font_from = str(from_to[0])\n",
        "                font_to = str(from_to[1])\n",
        "                grid_idx = str(grid_idx)\n",
        "                if len(font_from) == 1:\n",
        "                    font_from = '0' + font_from\n",
        "                if len(font_to) == 1:\n",
        "                    font_to = '0' + font_to\n",
        "                if len(grid_idx) == 1:\n",
        "                    grid_idx = '0' + grid_idx\n",
        "                idx = str(interpolated_font_ids.index(from_to))\n",
        "                if len(idx) == 1:\n",
        "                    idx = '0' + idx\n",
        "                file_path = '%s_from_%s_to_%s_grid_%s.png' % (idx, font_from, font_to, grid_idx)\n",
        "\n",
        "                # save\n",
        "                save_image(denorm_image(image.data), \\\n",
        "                           os.path.join(save_path, file_path), \\\n",
        "                           nrow=save_nrow, pad_value=255)\n",
        "    \n",
        "    return grid_results"
      ],
      "metadata": {
        "id": "N_owj04zPSZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "def batch_norm(c_out, momentum=0.1):\n",
        "    return nn.BatchNorm2d(c_out, momentum=momentum)\n",
        "\n",
        "\n",
        "def conv2d(c_in, c_out, k_size=3, stride=2, pad=1, dilation=1, bn=True, lrelu=True, leak=0.2):\n",
        "    layers = []\n",
        "    if lrelu:\n",
        "        layers.append(nn.LeakyReLU(leak))\n",
        "    layers.append(nn.Conv2d(c_in, c_out, k_size, stride, pad))\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def deconv2d(c_in, c_out, k_size=3, stride=1, pad=1, dilation=1, bn=True, dropout=False, p=0.5):\n",
        "    layers = []\n",
        "    layers.append(nn.LeakyReLU(0.2))\n",
        "    layers.append(nn.ConvTranspose2d(c_in, c_out, k_size, stride, pad))\n",
        "    if bn:\n",
        "        layers.append(nn.BatchNorm2d(c_out))\n",
        "    if dropout:\n",
        "        layers.append(nn.Dropout(p))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def lrelu(leak=0.2):\n",
        "    return nn.LeakyReLU(leak)\n",
        "\n",
        "\n",
        "def dropout(p=0.2):\n",
        "    return nn.Dropout(p)\n",
        "\n",
        "\n",
        "def fc(input_size, output_size):\n",
        "    return nn.Linear(input_size, output_size)\n",
        "    \n",
        "    \n",
        "def init_embedding(embedding_num, embedding_dim, stddev=0.01):\n",
        "    embedding = torch.randn(embedding_num, embedding_dim) * stddev\n",
        "    embedding = embedding.reshape((embedding_num, 1, 1, embedding_dim))\n",
        "    return embedding\n",
        "\n",
        "\n",
        "def embedding_lookup(embeddings, embedding_ids, GPU=False):\n",
        "    batch_size = len(embedding_ids)\n",
        "    embedding_dim = embeddings.shape[3]\n",
        "    local_embeddings = []\n",
        "    for id_ in embedding_ids:\n",
        "        if GPU:\n",
        "            local_embeddings.append(embeddings[id_].cpu().numpy())\n",
        "        else:\n",
        "            local_embeddings.append(embeddings[id_].data.numpy())\n",
        "    local_embeddings = torch.from_numpy(np.array(local_embeddings))\n",
        "    if GPU:\n",
        "        local_embeddings = local_embeddings.cuda()\n",
        "    local_embeddings = local_embeddings.reshape(batch_size, embedding_dim, 1, 1)\n",
        "    return local_embeddings\n",
        "\n",
        "\n",
        "def interpolated_embedding_lookup(embeddings, interpolated_embedding_ids, grid):\n",
        "    batch_size = len(interpolated_embedding_ids)\n",
        "    interpolated_embeddings = []\n",
        "    embedding_dim = embeddings.shape[3]\n",
        "\n",
        "    for id_ in interpolated_embedding_ids:\n",
        "        interpolated_embeddings.append((embeddings[id_[0]] * (1 - grid) + embeddings[id_[1]] * grid).cpu().numpy())\n",
        "    interpolated_embeddings = torch.from_numpy(np.array(interpolated_embeddings)).cuda()\n",
        "    interpolated_embeddings = interpolated_embeddings.reshape(batch_size, embedding_dim, 1, 1)\n",
        "    return interpolated_embeddings"
      ],
      "metadata": {
        "id": "4KMt7tqGQz0v"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}